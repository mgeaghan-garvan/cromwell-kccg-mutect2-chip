version 1.0

# =============================================================== #
# cromwell-kccg-mutect2-chip                                      #
#                                                                 #
# This workflow detects mutations associated with CHIP            #
# (clonal haematopoiesis of indeterminate potential) using the    #
# GATK4 Mutect2 somatic variant calling pipeline and the          #
# Cromwell workflow engine.                                       #
#                                                                 #
# This workflow has been adapted from the CHIP-detection-Mutect2  #
# workflow developed by Alex Bick. The Mutect2 somatic variant    #
# calling stage is adapted from the GATK best practices workflow, #
# and the CHIP detection stage is adapted from the                #
# Annovar Whitelist Filter developed by Charlie Condon.           #
#                                                                 #
# This pipeline has been developed for use by the Kinghorn        #
# Centre for Clinical Genomics and the Garvan Institute for       # 
# Medical Research.                                               #
#                                                                 #
# Author: Michael Geaghan (micgea)                                #
# Created: 2021/08/13                                             #
# =============================================================== #

# ========== LINKS ========== #
# CHIP-Detection-Mutect2 (Alex Bick): https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2
# Somatic variant calling best practices pipeline (Broad Institute): https://github.com/broadinstitute/gatk/blob/4.1.6.0/scripts/mutect2_wdl/mutect2.wdl
# Annovar Whitelist Filter (Charlie Condon): https://github.com/charliecondon/Annovar_Whitelist_Filter_WDL/tree/main
# =========================== #

# ========== Mutect2 ========== #
## Copyright Broad Institute, 2017
##
## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,
## and performs additional filtering tasks.
##
## Main requirements/expectations :
## - One analysis-ready BAM file (and its index) for each sample
##
## Description of inputs:
##
## ** Runtime **
## gatk_docker: docker image to use for GATK 4 Mutect2
## preemptible: how many preemptions to tolerate before switching to a non-preemptible machine (on Google)
## max_retries: how many times to retry failed tasks -- very important on the cloud when there are transient errors
## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file to be used instead of the GATK 4 jar
##                in the docker image.  This must be supplied when running in an environment that does not support docker
##                (e.g. SGE cluster on a Broad on-prem VM)
##
## ** Workflow options **
## intervals: genomic intervals (will be used for scatter)
## scatter_count: number of parallel jobs to generate when scattering over intervals
## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional)
## split_intervals_extra_args: additional arguments for splitting intervals before scattering (optional)
## run_orientation_bias_mixture_model_filter: (optional) if true, filter orientation bias sites with the read orientation artifact mixture model.
##
## ** Primary inputs **
## ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary
## tumor_bam, tumor_bam_index: BAM and index for the tumor sample
## normal_bam, normal_bam_index: BAM and index for the normal sample
##
## ** Primary resources ** (optional but strongly recommended)
## pon, pon_idx: optional panel of normals (and its index) in VCF format containing probable technical artifacts (false positves)
## gnomad, gnomad_idx: optional database of known germline variants (and its index) (see http://gnomad.broadinstitute.org/downloads)
## variants_for_contamination, variants_for_contamination_idx: VCF of common variants (and its index)with allele frequencies for calculating contamination
##
## ** Secondary resources ** (for optional tasks)
## realignment_index_bundle: resource for FilterAlignmentArtifacts, which runs if and only if it is specified.  Generated by BwaMemIndexImageCreator.
##
## Outputs :
## - One VCF file and its index with primary filtering applied; a bamout.bam
##   file of reassembled reads if requested
##
## Cromwell version support
## - Successfully tested on v34
##
## LICENSING :
## This script is released under the WDL source code license (BSD-3) (see LICENSE in
## https://github.com/broadinstitute/wdl). Note however that the programs it calls may
## be subject to different licenses. Users are responsible for checking that they are
## authorized to run all programs before running this script. Please see the docker
## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information
## pertaining to the included programs.

# ========== Annovar Whitelist Filter ========== #
## Version 8-2-2021
##
## This WDL workflow runs Annovar and a Whitelist Filter on the ouput VCFs from the Mutect2 Workflow.
##
##
## ** ANNOVAR **
## Annovar functionally annotates genetic variants detected from diverse genomes.
## Given a list of variants with chromosome, start position, end position, reference nucleotide
## and observed nucleotides, Annovar can perform gene-based annotation, region-based annotation,
## filter-based annotation, and more.
##
## See ANNOVAR documentation to fully understand functionality:
## https://annovar.openbioinformatics.org/en/latest/user-guide/startup/
##
## annovar_zip: the zipped folder with all of the needed files to run Annovar
##              NOTE: This file path is set on Terra - The file must be in the Workspace's bucket
## annovar_vcf_input: the Tables/sample column containing the vcf output files from a run of Mutect2
##                    NOTE: This is set on Terra (ex. this.filtered_vcf)
## annovar_protocols: the specificed protocols needed to run annovar (default = refGene,cosmic70)
##                    NOTE: You must add the needed file paths to annovar_data_sources
## annovar_operation: the specified operations needed to run annovar (default = g,f)
##                    NOTE: They must match up with annovar_protocols
## ref_name: the reference name needed for annovar to run (default = hg38)
## annovar_docker: the docker image to be used in the Annovar task
##
## ** WHITELIST_FILTER **
## WhitelistFilter filters annovar's output based on only relevant data to our lab's whitelist.
##
## You can find the R script code with comments on github: https://github.com/charliecondon/Annovar_Whitelist_Filter_WDL
##
## run_whitelist: if true, the WhitelistFilter task is run
## sample_id: set to the corresponding sample id for a given run
##			  NOTE: This is set on Terra (ex. this.sample_id)
## whitelist_filter_zip: the zipped folder with all of the files needed to run WhitelistFilter
##                       NOTE: This file path is set on Terra - The file must be in the Workspace's bucket
## txt_input: the txt input file that was an output of annovar
## whitelist_filter_docker: the docker image to be used in the Annovar task
##
## ** WDL OUTPUTS **
##  Four CSV files:
## 		- one with the whitelist filter applied
##		- one ready for manual review
##		- one with variant count information for debugging
##		- one with all the pre-whitelist variants listed
##
##
## Distributed under terms of the MIT License
## Copyright (c) 2021 Charlie Condon
## Contact <ccondon@vols.utk.edu>

struct Runtime {
    String gatk_docker
    File? gatk_override
    Int max_retries
    Int preemptible
    Int cpu
    Int machine_mem
    Int command_mem
    Int disk
    Int boot_disk_size
}

workflow Mutect2CHIP {
    input {
        # Mutect2 inputs
        File? intervals
        File ref_fasta
        File ref_fai
        File ref_dict
        File tumor_reads
        File tumor_reads_index
        File? normal_reads
        File? normal_reads_index
        File? pon
        File? pon_idx
        Int scatter_count = 10
        File? gnomad
        File? gnomad_idx
        File? variants_for_contamination
        File? variants_for_contamination_idx
        File? realignment_index_bundle
        String? realignment_extra_args
        Boolean? run_orientation_bias_mixture_model_filter
        String? m2_extra_args
        String? m2_extra_filtering_args
        String? split_intervals_extra_args
        Boolean? make_bamout
        Boolean? compress_vcfs
        File? gga_vcf
        File? gga_vcf_idx
        
        # VEP settings
        String vep_docker = "australia-southeast1-docker.pkg.dev/pb-dev-312200/somvar-images/vep@sha256:bc6a74bf271adb1484ea769660c7b69f5eea033d3ba2e2947988e6c5f034f221"  # :release_103.1
        String loftee_docker = "australia-southeast1-docker.pkg.dev/pb-dev-312200/somvar-images/vep-loftee@sha256:c95b78bacef4c8d3770642138e6f28998a5034cfad3fbef5451d2303c8c795d3"  # :vep_103.1_loftee_1.0.3
        Boolean vep = true
        String vep_species = "homo_sapiens"
        String vep_assembly = "GRCh38"
        File? vep_cache_archive
        Boolean loftee = true
        File? vep_loftee_ancestor_fa
        File? vep_loftee_ancestor_fai
        File? vep_loftee_ancestor_gzi
        File? vep_loftee_conservation_sql

        # Annovar settings
        Boolean annovar = false
        File? annovar_archive
        String annovar_assembly = "hg38"
        String annovar_docker = "australia-southeast1-docker.pkg.dev/pb-dev-312200/somvar-images/perl@sha256:1f35086e2ff48dace3b3edeaa2ad1faf1e44c0612e00f00ea0fc1830b576a261"  # :5.34.0
        String annovar_protocols = "cosmic70"
        String annovar_operations = "f"

        # Whitelist Filter settings
        Boolean run_chip_detection = true
        File? whitelist_filter_archive
        Boolean treat_missing_as_rare = true
        Boolean whitelist_genome = true
        String gnomad_pop = "AF"
        String whitelist_filter_docker = "australia-southeast1-docker.pkg.dev/pb-dev-312200/somvar-images/whitelist_filter@sha256:1f1f83f8241f40fbd1f21b19e2ccbdc184984fd9ec0b0a7bdfa97b8a73fed8a4"  # :latest

        # Samtools settings
        String samtools_docker = "australia-southeast1-docker.pkg.dev/pb-dev-312200/somvar-images/vep-loftee@sha256:c95b78bacef4c8d3770642138e6f28998a5034cfad3fbef5451d2303c8c795d3"  # same as loftee_docker

        # Runtime options
        String gatk_docker = "australia-southeast1-docker.pkg.dev/pb-dev-312200/somvar-images/gatk@sha256:0359ae4f32f2f541ca86a8cd30ef730bbaf8c306b9d53d2d520262d3e84b3b2b"  # :4.2.1.0
        File? gatk_override
        Int? preemptible
        Int? max_retries
        Int small_task_cpu = 4
        Int small_task_mem = 4000
        Int small_task_disk = 100
        Int command_mem_padding = 1000
        Boolean mem_per_core = true
        Int boot_disk_size = 12
        Int c2b_mem = 6000
        Int m2_mem = 5000
        Int m2_cpu = 4
        Int learn_read_orientation_mem = 5000
        Int filter_alignment_artifacts_mem = 5000
        Int vep_mem = 32000
        Int vep_cpu = 1

        # Use as a last resort to increase the disk given to every task in case of ill behaving data
        Int? emergency_extra_disk

        # These are multipliers to multipler inputs by to make sure we have enough disk to accommodate for possible output sizes
        # Large is for Bams/WGS vcfs
        # Small is for metrics/other vcfs
        Float large_input_to_output_multiplier = 2.25
        Float small_input_to_output_multiplier = 2.0
        Float cram_to_bam_multiplier = 6.0
    }

    Int preemptible_or_default = select_first([preemptible, 2])
    Int max_retries_or_default = select_first([max_retries, 2])

    Boolean compress = select_first([compress_vcfs, false])
    Boolean run_ob_filter = select_first([run_orientation_bias_mixture_model_filter, false])
    Boolean make_bamout_or_default = select_first([make_bamout, false])

    # Disk sizes used for dynamic sizing
    Int ref_size = ceil(size(ref_fasta, "GB") + size(ref_dict, "GB") + size(ref_fai, "GB"))
    Int tumor_reads_size = ceil(size(tumor_reads, "GB") + size(tumor_reads_index, "GB"))
    Int gnomad_vcf_size = if defined(gnomad) then ceil(size(gnomad, "GB")) else 0
    Int normal_reads_size = if defined(normal_reads) then ceil(size(normal_reads, "GB") + size(normal_reads_index, "GB")) else 0

    # If no tar is provided, the task downloads one from broads ftp server
    Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, "GB")) else 0

    # This is added to every task as padding, should increase if systematically you need more disk for every call
    Int disk_pad = 10 + gatk_override_size + select_first([emergency_extra_disk,0])

    # logic about output file names -- these are the names *without* .vcf extensions
    String output_basename = basename(basename(tumor_reads, ".bam"),".cram")  #hacky way to strip either .bam or .cram
    String unfiltered_name = output_basename + "-unfiltered"
    String filtered_name = output_basename + "-filtered"
    String output_vcf_name = output_basename + ".vcf"

    Int tumor_cram_to_bam_disk = ceil(tumor_reads_size * cram_to_bam_multiplier)
    Int normal_cram_to_bam_disk = ceil(normal_reads_size * cram_to_bam_multiplier)

    Int small_task_cpu_mult = if small_task_cpu > 1 then small_task_cpu - 1 else 1
    Int cmd_mem = if mem_per_core then (small_task_mem * small_task_cpu_mult) - command_mem_padding else small_task_mem - command_mem_padding

    Runtime standard_runtime = {
        "gatk_docker": gatk_docker,
        "gatk_override": gatk_override,
        "max_retries": max_retries_or_default,
        "preemptible": preemptible_or_default,
        "cpu": small_task_cpu,
        "machine_mem": small_task_mem,
        "command_mem": cmd_mem,
        "disk": small_task_disk + disk_pad,
        "boot_disk_size": boot_disk_size
    }

    # if (basename(tumor_reads) != basename(tumor_reads, ".cram")) {
    #     call CramToBam as TumorCramToBam {
    #         input:
    #             ref_fasta = ref_fasta,
    #             ref_fai = ref_fai,
    #             ref_dict = ref_dict,
    #             cram = tumor_reads,
    #             crai = tumor_reads_index,
    #             name = output_basename,
    #             samtools_docker = samtools_docker,
    #             disk_size_gb = tumor_cram_to_bam_disk,
    #             mem_mb = c2b_mem
    #     }
    # }

    # if (defined(normal_reads)) {
    #     String normal_or_empty = select_first([normal_reads, ""])
    #     if (basename(normal_or_empty) != basename(normal_or_empty, ".cram")) {
    #         String normal_basename = basename(basename(normal_or_empty, ".bam"),".cram")
    #         call CramToBam as NormalCramToBam {
    #             input:
    #                 ref_fasta = ref_fasta,
    #                 ref_fai = ref_fai,
    #                 ref_dict = ref_dict,
    #                 cram = normal_reads,
    #                 crai = normal_reads_index,
    #                 name = normal_basename,
    #                 samtools_docker = samtools_docker,
    #                 disk_size_gb = normal_cram_to_bam_disk,
    #                 mem_mb = c2b_mem
    #         }
    #     }
    # }

    # File tumor_bam = select_first([TumorCramToBam.output_bam, tumor_reads])
    # File tumor_bai = select_first([TumorCramToBam.output_bai, tumor_reads_index])
    File tumor_bam = tumor_reads
    File tumor_bai = tumor_reads_index
    Int tumor_bam_size = ceil(size(tumor_bam, "GB") + size(tumor_bai, "GB"))

    # File? normal_bam = if defined(normal_reads) then select_first([NormalCramToBam.output_bam, normal_reads]) else normal_reads
    # File? normal_bai = if defined(normal_reads) then select_first([NormalCramToBam.output_bai, normal_reads_index]) else normal_reads_index
    File? normal_bam = normal_reads
    File? normal_bai = normal_reads_index
    Int normal_bam_size = if defined(normal_bam) then ceil(size(normal_bam, "GB") + size(normal_bai, "GB")) else 0

    Int m2_output_size = tumor_bam_size / scatter_count
    #TODO: do we need to change this disk size now that NIO is always going to happen (for the google backend only)
    Int m2_per_scatter_size = (tumor_bam_size + normal_bam_size) + ref_size + gnomad_vcf_size + m2_output_size + disk_pad

    call SplitIntervals {
        input:
            intervals = intervals,
            ref_fasta = ref_fasta,
            ref_fai = ref_fai,
            ref_dict = ref_dict,
            scatter_count = scatter_count,
            split_intervals_extra_args = split_intervals_extra_args,
            runtime_params = standard_runtime
    }

    scatter (subintervals in SplitIntervals.interval_files ) {
        call M2 {
            input:
                intervals = subintervals,
                ref_fasta = ref_fasta,
                ref_fai = ref_fai,
                ref_dict = ref_dict,
                tumor_bam = tumor_bam,
                tumor_bai = tumor_bai,
                normal_bam = normal_bam,
                normal_bai = normal_bai,
                pon = pon,
                pon_idx = pon_idx,
                gnomad = gnomad,
                gnomad_idx = gnomad_idx,
                preemptible = preemptible,
                max_retries = max_retries,
                m2_extra_args = m2_extra_args,
                variants_for_contamination = variants_for_contamination,
                variants_for_contamination_idx = variants_for_contamination_idx,
                make_bamout = make_bamout_or_default,
                run_ob_filter = run_ob_filter,
                compress = compress,
                gga_vcf = gga_vcf,
                gga_vcf_idx = gga_vcf_idx,
                gatk_override = gatk_override,
                gatk_docker = gatk_docker,
                disk_space = m2_per_scatter_size,
                mem_mb = m2_mem,
                mem_pad = command_mem_padding,
                mem_per_core = mem_per_core,
                cpu = m2_cpu
        }
    }

    Int merged_vcf_size = ceil(size(M2.unfiltered_vcf, "GB"))
    Int merged_bamout_size = ceil(size(M2.output_bamOut, "GB"))

    if (run_ob_filter) {
        call LearnReadOrientationModel {
            input:
                f1r2_tar_gz = M2.f1r2_counts,
                runtime_params = standard_runtime,
                output_name = output_basename,
                mem_mb = learn_read_orientation_mem,
                mem_pad = command_mem_padding,
                mem_per_core = mem_per_core
        }
    }

    call MergeVCFs {
        input:
            input_vcfs = M2.unfiltered_vcf,
            input_vcf_indices = M2.unfiltered_vcf_idx,
            output_name = unfiltered_name,
            compress = compress,
            runtime_params = standard_runtime
    }

    if (make_bamout_or_default) {
        call MergeBamOuts {
            input:
                ref_fasta = ref_fasta,
                ref_fai = ref_fai,
                ref_dict = ref_dict,
                bam_outs = M2.output_bamOut,
                output_vcf_name = basename(MergeVCFs.merged_vcf, ".vcf"),
                runtime_params = standard_runtime,
                disk_space = ceil(merged_bamout_size * large_input_to_output_multiplier) + disk_pad,
        }
    }

    call MergeStats {
        input:
            stats = M2.stats,
            output_name = output_basename,
            runtime_params = standard_runtime
    }

    if (defined(variants_for_contamination)) {
        call MergePileupSummaries as MergeTumorPileups {
            input:
                input_tables = flatten(M2.tumor_pileups),
                output_name = output_basename,
                ref_dict = ref_dict,
                runtime_params = standard_runtime
        }

        if (defined(normal_bam)){
            call MergePileupSummaries as MergeNormalPileups {
                input:
                    input_tables = flatten(M2.normal_pileups),
                    output_name = output_basename,
                    ref_dict = ref_dict,
                    runtime_params = standard_runtime
            }
        }

        call CalculateContamination {
            input:
                tumor_pileups = MergeTumorPileups.merged_table,
                normal_pileups = MergeNormalPileups.merged_table,
                output_name = output_basename,
                runtime_params = standard_runtime
        }
    }

    call Filter {
        input:
            ref_fasta = ref_fasta,
            ref_fai = ref_fai,
            ref_dict = ref_dict,
            intervals = intervals,
            unfiltered_vcf = MergeVCFs.merged_vcf,
            unfiltered_vcf_idx = MergeVCFs.merged_vcf_idx,
            output_name = filtered_name,
            compress = compress,
            mutect_stats = MergeStats.merged_stats,
            contamination_table = CalculateContamination.contamination_table,
            maf_segments = CalculateContamination.maf_segments,
            artifact_priors_tar_gz = LearnReadOrientationModel.artifact_prior_table,
            m2_extra_filtering_args = m2_extra_filtering_args,
            runtime_params = standard_runtime,
            disk_space = ceil(size(MergeVCFs.merged_vcf, "GB") * small_input_to_output_multiplier) + disk_pad
    }

    # FilterAlignmentArtifacts is experimental and not recommended for production use
    # There is also a bug causing issues when running on the Garvan HPC
    # This may be an issue with the underlying C library, and may be hardware-specific, as the error is similar to https://gatk.broadinstitute.org/hc/en-us/community/posts/360062334692-FilterAlignmentArtifacts-error
    # More testing is required.
    # TODO: Figure out why this isn't working. Low priority - experimental feature.
    if (defined(realignment_index_bundle)) {
        call FilterAlignmentArtifacts {
            input:
                ref_fasta = ref_fasta,
                ref_fai = ref_fai,
                ref_dict = ref_dict,
                bam = tumor_bam,
                bai = tumor_bai,
                realignment_index_bundle = select_first([realignment_index_bundle]),
                realignment_extra_args = realignment_extra_args,
                compress = compress,
                output_name = filtered_name,
                input_vcf = Filter.filtered_vcf,
                input_vcf_idx = Filter.filtered_vcf_idx,
                runtime_params = standard_runtime,
                mem_mb = filter_alignment_artifacts_mem,
                mem_pad = command_mem_padding,
                mem_per_core = mem_per_core
        }
    }

    File filter_output_vcf = select_first([FilterAlignmentArtifacts.filtered_vcf, Filter.filtered_vcf])
    File filter_output_vcf_idx = select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx])

    if (vep && defined(vep_cache_archive)) {
        File vep_cache_archive_file = select_first([vep_cache_archive, "CACHE_FILE_NOT_SUPPLIED"])
        File vep_input_vcf = filter_output_vcf  # select_first([FilterAlignmentArtifacts.filtered_vcf, Filter.filtered_vcf])
        File vep_input_vcf_idx = filter_output_vcf_idx  # select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx])
        Int n_vep_cpus = if loftee then 1 else vep_cpu

        call VEP {
            input:
                input_vcf = vep_input_vcf,
                input_vcf_idx = vep_input_vcf_idx,
                species = vep_species,
                assembly = vep_assembly,
                cache_archive = vep_cache_archive_file,
                fasta = ref_fasta,
                vep_docker = vep_docker,
                loftee_docker = loftee_docker,
                loftee = loftee,
                loftee_ancestor_fa = vep_loftee_ancestor_fa,
                loftee_ancestor_fai = vep_loftee_ancestor_fai,
                loftee_ancestor_gzi = vep_loftee_ancestor_gzi,
                loftee_conservation_sql = vep_loftee_conservation_sql,
                mem_mb = vep_mem,
                cpus = n_vep_cpus,
                runtime_params = standard_runtime
        }
    }

    File annovar_archive_file = select_first([annovar_archive, "ANNOVAR_ARCHIVE_NOT_SUPPLIED"])
    File annovar_input_vcf = select_first([VEP.output_vcf, filter_output_vcf])

    if (annovar && defined(annovar_archive)) {
        String annovar_sample_id = basename(basename(annovar_input_vcf, ".gz"), ".vcf")
        call Annovar {
            input:
                mem_mb = 4000,
                annovar_disk_space = 300,
                cpu = 1,
                annovar_docker = annovar_docker,
                sample_id = annovar_sample_id,
                vcf_input = annovar_input_vcf,
                annovar_archive = annovar_archive_file,
                ref_name = annovar_assembly,
                annovar_protocols = annovar_protocols,
                annovar_operations = annovar_operations,
                runtime_params = standard_runtime
        }
    }

    File chip_detection_input_vcf = select_first([Annovar.annovar_output_file_vcf, annovar_input_vcf])

    # Run annovar and CHIP whitelist filter
    # TODO: change mem_mb, *_disk_space, and cpu to workflow input parameters
    if (run_chip_detection && defined(annovar_archive) && defined(whitelist_filter_archive)) {
        # File annovar_archive_file = select_first([annovar_archive, "ANNOVAR_ARCHIVE_NOT_SUPPLIED"])
        File whitelist_filter_archive_file = select_first([whitelist_filter_archive, "WHITELIST_FILTER_ARCHIVE_NOT_SUPPLIED"])
        String sample_id = basename(basename(chip_detection_input_vcf, ".gz"), ".vcf")
        String tumor_sample_name = M2.tumor_sample[0]  # M2 is scattered over intervals, all entries in the Array[String] "tumor_sample" will be identical
        String whitelist_annovar_protocols = if (whitelist_genome) then "refGene,gnomad211_genome,gnomad211_exome" else "refGene,gnomad211_exome"
        String whitelist_annovar_operations = if (whitelist_genome) then "g,f,f" else "g,f"
        call Annovar as WhitelistAnnovar {
            input:
                mem_mb = 4000,
                annovar_disk_space = 300,
                cpu = 1,
                annovar_docker = annovar_docker,
                sample_id = sample_id,
                vcf_input = chip_detection_input_vcf,
                annovar_archive = annovar_archive_file,
                ref_name = annovar_assembly,
                label = "whitelist_annovar_out",
                annovar_protocols = whitelist_annovar_protocols,
                annovar_operations = whitelist_annovar_operations,
                runtime_params = standard_runtime
        }

        String whitelist_exome_only_or_both = if (whitelist_genome) then "genome,exome" else "exome"

        call WhitelistFilter {
            input:
                mem_mb = 10000,
                whitelist_filter_disk_space = 300,
                cpu = 1,
                whitelist_filter_docker = whitelist_filter_docker,
                tumor_sample_name = tumor_sample_name,
                treat_missing_as_rare = treat_missing_as_rare,
                gnomad_source = whitelist_exome_only_or_both,
                gnomad_pop = gnomad_pop,
                txt_input = WhitelistAnnovar.annovar_output_file_table,
                vcf_input = WhitelistAnnovar.annovar_output_file_vcf,
                ref_name = annovar_assembly,
                ref_fasta = ref_fasta,
                whitelist_filter_archive = whitelist_filter_archive_file,
                runtime_params = standard_runtime
        }
    }

    output {
        File filtered_vcf = filter_output_vcf  # select_first([FilterAlignmentArtifacts.filtered_vcf, Filter.filtered_vcf])
        File filtered_vcf_idx = filter_output_vcf_idx  # select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx])
        File filtering_stats = Filter.filtering_stats
        File mutect_stats = MergeStats.merged_stats
        File? contamination_table = CalculateContamination.contamination_table
        File? bamout = MergeBamOuts.merged_bam_out
        File? bamout_index = MergeBamOuts.merged_bam_out_index
        File? maf_segments = CalculateContamination.maf_segments
        File? read_orientation_model_params = LearnReadOrientationModel.artifact_prior_table
        File? out_vep_vcf = VEP.output_vcf
        File? out_annovar_vcf = Annovar.annovar_output_file_vcf
        File? out_annovar_table = Annovar.annovar_output_file_table
        File? out_whitelist_annovar_vcf = WhitelistAnnovar.annovar_output_file_vcf
        File? out_whitelist_annovar_table = WhitelistAnnovar.annovar_output_file_table
        File? out_whitelist_filter_output_allvariants_csv = WhitelistFilter.whitelist_filter_output_allvariants_csv
        File? out_whitelist_filter_output_wl_csv = WhitelistFilter.whitelist_filter_output_wl_csv
        File? out_whitelist_filter_output_manual_review_csv = WhitelistFilter.whitelist_filter_output_manual_review_csv
        File? out_whitelist_filter_output_putative_wl_csv = WhitelistFilter.whitelist_filter_output_putative_wl_csv
        File? out_whitelist_filter_output_putative_manual_review_csv = WhitelistFilter.whitelist_filter_output_putative_manual_review_csv
    }
}

# ================ #
# TASK DEFINITIONS #
# ================ #

task CramToBam {
    input {
      File ref_fasta
      File ref_fai
      File ref_dict
      # cram and crai must be optional since Normal cram is optional
      File? cram
      File? crai
      String name
      String samtools_docker
      Int disk_size_gb
      Int mem_mb = 6000
    }

    # DNAnexus compatability: get the filename of all optional index files
    String crai_def = if defined(crai) then "defined" else "undefined"

    # Calls samtools view to do the conversion
    command {
        # Set -e and -o says if any command I run fails in this script, make sure to return a failure
        set -e
        set -o pipefail

        # DNAnexus compatability: utilise optional index filenames to ensure they get localized
        OPT_VAR_DEFINED="~{crai_def}"

        samtools view -h -T ~{ref_fasta} ~{cram} |
            samtools view -b -o ~{name}.bam -
        samtools index -b ~{name}.bam
        mv ~{name}.bam.bai ~{name}.bai
    }

    runtime {
        docker: samtools_docker
        memory: mem_mb + " MB"
        mem_mb: mem_mb
        disks: "local-disk " + disk_size_gb + " HDD"
    }

    output {
        File output_bam = "~{name}.bam"
        File output_bai = "~{name}.bai"
    }
}

task SplitIntervals {
    input {
      File? intervals
      File ref_fasta
      File ref_fai
      File ref_dict
      Int scatter_count
      String? split_intervals_extra_args

      # runtime
      Runtime runtime_params
    }

    command {
        set -e
        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

        mkdir interval-files
        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" SplitIntervals \
            -R ~{ref_fasta} \
            ~{"-L " + intervals} \
            -scatter ~{scatter_count} \
            -O interval-files \
            ~{split_intervals_extra_args}
        cp interval-files/*.interval_list .
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        Array[File] interval_files = glob("*.interval_list")
    }
}

task M2 {
    input {
      File? intervals
      File ref_fasta
      File ref_fai
      File ref_dict
      File tumor_bam
      File tumor_bai
      File? normal_bam
      File? normal_bai
      File? pon
      File? pon_idx
      File? gnomad
      File? gnomad_idx
      String? m2_extra_args
      Boolean? make_bamout
      Boolean? run_ob_filter
      Boolean compress
      File? gga_vcf
      File? gga_vcf_idx
      File? variants_for_contamination
      File? variants_for_contamination_idx
      File? gatk_override
      # runtime
      String gatk_docker
      Int mem_mb = 5000
      Int mem_pad = 1000
      Boolean mem_per_core = true
      Int? preemptible
      Int? max_retries
      Int? disk_space
      Int cpu = 4
      Boolean use_ssd = false
    }

    String output_vcf = "output" + if compress then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress then ".tbi" else ".idx"

    String output_stats = output_vcf + ".stats"

    Int machine_mem = mem_mb
    Int cpu_mult = if cpu > 1 then cpu - 1 else 1
    Int command_mem = if mem_per_core then (machine_mem * cpu_mult) - mem_pad else machine_mem - mem_pad

    # DNAnexus compatability: get the filename of all optional index files
    String normal_bai_def = if defined(normal_bai) then "defined" else "undefined"
    String pon_idx_def = if defined(pon_idx) then "defined" else "undefined"
    String gnomad_idx_def = if defined(gnomad_idx) then "defined" else "undefined"
    String gga_vcf_idx_def = if defined(gga_vcf_idx) then "defined" else "undefined"
    String variants_for_contamination_idx_def = if defined(variants_for_contamination_idx) then "defined" else "undefined"

    parameter_meta{
      intervals: {localization_optional: true}
      ref_fasta: {localization_optional: true}
      ref_fai: {localization_optional: true}
      ref_dict: {localization_optional: true}
      tumor_bam: {localization_optional: true}
      tumor_bai: {localization_optional: true}
      normal_bam: {localization_optional: true}
      normal_bai: {localization_optional: true}
      pon: {localization_optional: true}
      pon_idx: {localization_optional: true}
      gnomad: {localization_optional: true}
      gnomad_idx: {localization_optional: true}
      gga_vcf: {localization_optional: true}
      gga_vcf_idx: {localization_optional: true}
      variants_for_contamination: {localization_optional: true}
      variants_for_contamination_idx: {localization_optional: true}
    }

    command <<<
        set -e

        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" gatk_override}

        # We need to create these files regardless, even if they stay empty
        touch bamout.bam
        touch f1r2.tar.gz
        echo "" > normal_name.txt

        # DNAnexus compatability: echo optional index filenames to ensure they get localized
        OPT_VAR_DEFINED="~{normal_bai_def}"
        OPT_VAR_DEFINED="~{pon_idx_def}"
        OPT_VAR_DEFINED="~{gnomad_idx_def}"
        OPT_VAR_DEFINED="~{gga_vcf_idx_def}"
        OPT_VAR_DEFINED="~{variants_for_contamination_idx_def}"

        gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" GetSampleName -R ~{ref_fasta} -I ~{tumor_bam} -O tumor_name.txt -encode
        tumor_command_line="-I ~{tumor_bam} -tumor `cat tumor_name.txt`"

        if [[ ! -z "~{normal_bam}" ]]; then
            gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" GetSampleName -R ~{ref_fasta} -I ~{normal_bam} -O normal_name.txt -encode
            normal_command_line="-I ~{normal_bam} -normal `cat normal_name.txt`"
        fi

        gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" Mutect2 \
            -R ~{ref_fasta} \
            $tumor_command_line \
            $normal_command_line \
            ~{"--germline-resource " + gnomad} \
            ~{"-pon " + pon} \
            ~{"-L " + intervals} \
            ~{"--alleles " + gga_vcf} \
            -O "~{output_vcf}" \
            ~{true='--bam-output bamout.bam' false='' select_first([make_bamout, false])} \
            ~{true='--f1r2-tar-gz f1r2.tar.gz' false='' select_first([run_ob_filter, false])} \
            ~{m2_extra_args}

        m2_exit_code=$?

        ### GetPileupSummaries

        # If the variants for contamination and the intervals for this scatter don't intersect, GetPileupSummaries
        # throws an error.  However, there is nothing wrong with an empty intersection for our purposes; it simply doesn't
        # contribute to the merged pileup summaries that we create downstream.  We implement this by with array outputs.
        # If the tool errors, no table is created and the glob yields an empty array.
        set +e

        if [[ ! -z "~{variants_for_contamination}" ]]; then
            gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" GetPileupSummaries -R ~{ref_fasta} -I ~{tumor_bam} ~{"--interval-set-rule INTERSECTION -L " + intervals} \
                -V ~{variants_for_contamination} -L ~{variants_for_contamination} -O tumor-pileups.table

            if [[ ! -z "~{normal_bam}" ]]; then
                gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" GetPileupSummaries -R ~{ref_fasta} -I ~{normal_bam} ~{"--interval-set-rule INTERSECTION -L " + intervals} \
                    -V ~{variants_for_contamination} -L ~{variants_for_contamination} -O normal-pileups.table
            fi
        fi

        # the script only fails if Mutect2 itself fails
        exit $m2_exit_code
    >>>

    runtime {
        docker: gatk_docker
        bootDiskSizeGb: 12
        memory: machine_mem + " MB"
        mem_mb: machine_mem
        disks: "local-disk " + select_first([disk_space, 100]) + if use_ssd then " SSD" else " HDD"
        preemptible: select_first([preemptible, 10])
        maxRetries: select_first([max_retries, 0])
        cpu: select_first([cpu, 4])
    }

    output {
        File unfiltered_vcf = "~{output_vcf}"
        File unfiltered_vcf_idx = "~{output_vcf_idx}"
        File output_bamOut = "bamout.bam"
        String tumor_sample = read_string("tumor_name.txt")
        String normal_sample = read_string("normal_name.txt")
        File stats = "~{output_stats}"
        File f1r2_counts = "f1r2.tar.gz"
        Array[File] tumor_pileups = glob("*tumor-pileups.table")
        Array[File] normal_pileups = glob("*normal-pileups.table")
    }
}

# Learning step of the orientation bias mixture model, which is the recommended orientation bias filter as of September 2018
task LearnReadOrientationModel {
    input {
      Array[File] f1r2_tar_gz
      Runtime runtime_params
      String output_name
      Int mem_mb = 5000
      Int mem_pad = 1000
      Boolean mem_per_core = true
    }

    Int machine_mem = mem_mb
    Int cpu_mult = if runtime_params.cpu > 1 then runtime_params.cpu - 1 else 1
    Int command_mem = if mem_per_core then (machine_mem * cpu_mult) - mem_pad else machine_mem - mem_pad

    command {
        set -e
        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

        gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" LearnReadOrientationModel \
            -I ~{sep=" -I " f1r2_tar_gz} \
            -O "~{output_name}-artifact-priors.tar.gz"
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: machine_mem + " MB"
        mem_mb: machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File artifact_prior_table = "~{output_name}-artifact-priors.tar.gz"
    }

}

task MergeVCFs {
    input {
      Array[File] input_vcfs
      Array[File] input_vcf_indices
      String output_name
      Boolean compress
      Runtime runtime_params
    }

    String output_vcf = output_name + if compress then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress then ".tbi" else ".idx"

    # using MergeVcfs instead of GatherVcfs so we can create indices
    # WARNING 2015-10-28 15:01:48 GatherVcfs  Index creation not currently supported when gathering block compressed VCFs.
    command {
        set -e
        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" MergeVcfs -I ~{sep=' -I ' input_vcfs} -O ~{output_vcf}
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File merged_vcf = "~{output_vcf}"
        File merged_vcf_idx = "~{output_vcf_idx}"
    }
}

task MergeBamOuts {
    input {
      File ref_fasta
      File ref_fai
      File ref_dict
      Array[File]+ bam_outs
      String output_vcf_name
      Runtime runtime_params
      Int? disk_space   #override to request more disk than default small task params
    }

    command <<<
        # This command block assumes that there is at least one file in bam_outs.
        #  Do not call this task if len(bam_outs) == 0
        set -e
        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" GatherBamFiles \
            -I ~{sep=" -I " bam_outs} -O unsorted.out.bam -R ~{ref_fasta}

        # We must sort because adjacent scatters may have overlapping (padded) assembly regions, hence
        # overlapping bamouts

        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" SortSam -I unsorted.out.bam \
            -O ~{output_vcf_name}.out.bam \
            --SORT_ORDER coordinate -VALIDATION_STRINGENCY LENIENT
        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" BuildBamIndex -I ~{output_vcf_name}.out.bam -VALIDATION_STRINGENCY LENIENT
    >>>

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + select_first([disk_space, runtime_params.disk]) + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File merged_bam_out = "~{output_vcf_name}.out.bam"
        File merged_bam_out_index = "~{output_vcf_name}.out.bai"
    }
}

task MergeStats {
    input {
      Array[File]+ stats
      String output_name
      Runtime runtime_params
    }

    command {
        set -e
        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}


        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" MergeMutectStats \
            -stats ~{sep=" -stats " stats} -O "~{output_name}-merged.stats"
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File merged_stats = "~{output_name}-merged.stats"
    }
}

task MergePileupSummaries {
    input {
      Array[File] input_tables
      String output_name
      File ref_dict
      Runtime runtime_params
    }

    command {
        set -e
        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" GatherPileupSummaries \
        --sequence-dictionary ~{ref_dict} \
        -I ~{sep=' -I ' input_tables} \
        -O ~{output_name}.tsv
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File merged_table = "~{output_name}.tsv"
    }
}

task CalculateContamination {
    input {
      String? intervals
      File tumor_pileups
      File? normal_pileups
      String output_name
      Runtime runtime_params
    }

    command {
        set -e

        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" CalculateContamination -I ~{tumor_pileups} \
        -O "~{output_name}-contamination.table" --tumor-segmentation "~{output_name}-segments.table" ~{"-matched " + normal_pileups}
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File contamination_table = "~{output_name}-contamination.table"
        File maf_segments = "~{output_name}-segments.table"
    }
}

task Filter {
    input {
      File? intervals
      File ref_fasta
      File ref_fai
      File ref_dict
      File unfiltered_vcf
      File unfiltered_vcf_idx
      String output_name
      Boolean compress
      File? mutect_stats
      File? artifact_priors_tar_gz
      File? contamination_table
      File? maf_segments
      String? m2_extra_filtering_args

      Runtime runtime_params
      Int? disk_space
    }

    String output_vcf = output_name + if compress then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress then ".tbi" else ".idx"

    parameter_meta{
      ref_fasta: {localization_optional: true}
      ref_fai: {localization_optional: true}
      ref_dict: {localization_optional: true}
    }

    command {
        set -e

        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

        gatk --java-options "-Xmx~{runtime_params.command_mem}m -Xms~{runtime_params.command_mem - 1000}m" FilterMutectCalls -V ~{unfiltered_vcf} \
            -R ~{ref_fasta} \
            -O ~{output_vcf} \
            ~{"--contamination-table " + contamination_table} \
            ~{"--tumor-segmentation " + maf_segments} \
            ~{"--ob-priors " + artifact_priors_tar_gz} \
            ~{"-stats " + mutect_stats} \
            --filtering-stats "~{output_name}.filtering.stats" \
            ~{m2_extra_filtering_args}
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: runtime_params.machine_mem + " MB"
        mem_mb: runtime_params.machine_mem
        disks: "local-disk " + select_first([disk_space, runtime_params.disk]) + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File filtered_vcf = "~{output_vcf}"
        File filtered_vcf_idx = "~{output_vcf_idx}"
        File filtering_stats = "~{output_name}.filtering.stats"
    }
}

task FilterAlignmentArtifacts {
    input {
      File ref_fasta
      File ref_fai
      File ref_dict
      File input_vcf
      File input_vcf_idx
      File bam
      File bai
      String output_name
      Boolean compress
      File realignment_index_bundle
      String? realignment_extra_args
      Runtime runtime_params
      Int mem_mb = 5000
      Int mem_pad = 1000
      Boolean mem_per_core = true
    }

    String output_vcf = output_name + if compress then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf +  if compress then ".tbi" else ".idx"

    Int machine_mem = mem_mb
    Int cpu_mult = if runtime_params.cpu > 1 then runtime_params.cpu - 1 else 1
    Int command_mem = if mem_per_core then (machine_mem * cpu_mult) - mem_pad else machine_mem - mem_pad

    parameter_meta{
      ref_fasta: {localization_optional: true}
      ref_fai: {localization_optional: true}
      ref_dict: {localization_optional: true}
      input_vcf: {localization_optional: true}
      input_vcf_idx: {localization_optional: true}
      bam: {localization_optional: true}
      bai: {localization_optional: true}
    }

    command {
        set -e

        export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

        gatk --java-options "-Xmx~{command_mem}m -Xms~{command_mem - 1000}m" FilterAlignmentArtifacts \
            -R ~{ref_fasta} \
            -V ~{input_vcf} \
            -I ~{bam} \
            --bwa-mem-index-image ~{realignment_index_bundle} \
            ~{realignment_extra_args} \
            -O ~{output_vcf}
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: machine_mem + " MB"
        mem_mb: machine_mem
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }

    output {
        File filtered_vcf = "~{output_vcf}"
        File filtered_vcf_idx = "~{output_vcf_idx}"
    }
}

task VEP {
    input {
        # Need to be optional since the input file depends on whether realignment artifacts have been filtered
        File input_vcf
        File input_vcf_idx
        String species = "homo_sapiens"
        String assembly = "GRCh38"
        File cache_archive
        File fasta
        String vep_docker
        String loftee_docker
        Boolean loftee = true
        File? loftee_ancestor_fa
        File? loftee_ancestor_fai
        File? loftee_ancestor_gzi
        File? loftee_conservation_sql
        Boolean offline = true
        Int cpus = 1
        Int mem_mb = 32000
        Int? buffer_size
        Int loftee_buffer_size = 1
        Runtime runtime_params
    }

    String output_options = "--vcf --no_stats"
    String input_basename = basename(basename(input_vcf, ".gz"), ".vcf")
    String output_suffix = ".vep.vcf"
    String output_file = input_basename + output_suffix
    String stats_file = input_basename + ".vep.html"
    String offline_options = if offline then "--offline" else ""

    # DNAnexus compatability: get the filename of all optional index files
    String loftee_ancestor_fai_def = if defined(loftee_ancestor_fai) then "defined" else "undefined"
    String loftee_ancestor_gzi_def = if defined(loftee_ancestor_gzi) then "defined" else "undefined"


    command {
        # DNAnexus compatability: echo optional index filenames to ensure they get localized
        OPT_VAR_DEFINED="~{loftee_ancestor_fai_def}"
        OPT_VAR_DEFINED="~{loftee_ancestor_gzi_def}"

        mkdir -p /tmp/.vep/cache
        tar -xzvf ~{cache_archive} -C /tmp/.vep/cache/
        # this seems necessary on GCP - running into permissions errors.
        # TODO: find a better solution
        cp ~{fasta} /tmp/ref_fasta.fasta
        vep \
            --dir_cache /tmp/.vep/cache \
            --dir_plugins /plugins/loftee-1.0.3 \
            -i ~{input_vcf} \
            --species ~{species} \
            --assembly ~{assembly} \
            ~{output_options} \
            -o ~{output_file} \
            --stats_file ~{stats_file} \
            ~{offline_options} \
            --cache \
            --fasta /tmp/ref_fasta.fasta \
            ~{if loftee then "" else "--fork " + cpus} \
            ~{if loftee then "--buffer_size " + loftee_buffer_size else if defined(buffer_size) then "--buffer_size " + select_first([buffer_size, 1]) else ""} \
            --no_progress \
            --everything \
            --pubmed \
            --hgvsg \
            --shift_hgvs 1 \
            ~{if loftee then "--plugin LoF,loftee_path:/plugins/loftee-1.0.3,human_ancestor_fa:" + loftee_ancestor_fa + ",conservation_file:" + loftee_conservation_sql else ""}
    }

    String docker_to_use = if (loftee && loftee_docker != "") then loftee_docker else vep_docker
    runtime {
        docker: docker_to_use
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: mem_mb + " MB"
        mem_mb: mem_mb
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: cpus
    }

    output {
        File? output_vcf = "~{input_basename}.vep.vcf"
    }
}

task Annovar {
    input {
      Int mem_mb = 4000
      Int annovar_disk_space = 300
      Int cpu = 1
      String annovar_docker

      String sample_id
      File vcf_input
      File annovar_archive

      String label = "annovar_out"

      String ref_name = "hg38"
      String annovar_protocols = "cosmic70"
      String annovar_operations = "f"
      Runtime runtime_params
    }

    String file_prefix = sample_id + "." + label

    command {
      set -euo pipefail

      cd /tmp

      tar -xzvf ~{annovar_archive}

      cd -

      chmod +x /tmp/annovar_files/convert2annovar.pl
      chmod +x /tmp/annovar_files/table_annovar.pl
      chmod +x /tmp/annovar_files/annotate_variation.pl
      chmod +x /tmp/annovar_files/coding_change.pl
      chmod +x /tmp/annovar_files/retrieve_seq_from_fasta.pl
      chmod +x /tmp/annovar_files/variants_reduction.pl

      perl /tmp/annovar_files/table_annovar.pl ~{vcf_input} /tmp/annovar_files \
        -buildver ~{default="hg38" ref_name} \
        -out ~{file_prefix} \
        -remove \
        -protocol ~{annovar_protocols} \
        -operation ~{annovar_operations} \
        -nastring . -vcfinput
    }

    runtime {
      docker: annovar_docker
      bootDiskSizeGb: runtime_params.boot_disk_size
      memory: mem_mb + " MB"
      mem_mb: mem_mb
      disks: "local-disk " + annovar_disk_space + " HDD"
      preemptible: runtime_params.preemptible
      maxRetries: runtime_params.max_retries
      cpu: cpu
    }

    output {
      File annovar_output_file_vcf = file_prefix + ".hg38_multianno.vcf"
      File annovar_output_file_table = file_prefix + ".hg38_multianno.txt"
    }
}

task WhitelistFilter {
    input {
      Int mem_mb = 10000
      Int whitelist_filter_disk_space = 300
      Int cpu = 1
      String whitelist_filter_docker
      String tumor_sample_name
      String gnomad_source = "genome,exome"
      String gnomad_pop = "AF"
      Boolean treat_missing_as_rare = true
      File txt_input
      File vcf_input
      String ref_name
      File ref_fasta
      File whitelist_filter_archive
      Runtime runtime_params
    }

    String file_prefix = basename(txt_input, "_multianno.txt")
    String treat_missing_as_rare_str = if (treat_missing_as_rare) then "TRUE" else "FALSE"

    command {
      set -euo pipefail

      tar -xzvf ~{whitelist_filter_archive}

      Rscript ./whitelist_filter_files/whitelist_filter_rscript.R ~{txt_input} ~{vcf_input} ~{tumor_sample_name} ~{gnomad_source} ~{gnomad_pop} ~{treat_missing_as_rare_str} ./whitelist_filter_files/chip_variant_definitions.csv ./whitelist_filter_files/transcript_protein_lengths.csv ~{ref_fasta}
    }

    runtime {
      docker: whitelist_filter_docker
      bootDiskSizeGb: runtime_params.boot_disk_size
      memory: mem_mb + " MB"
      mem_mb: mem_mb
      disks: "local-disk " + whitelist_filter_disk_space + " HDD"
      preemptible: runtime_params.preemptible
      maxRetries: runtime_params.max_retries
      cpu: cpu
    }

    output {
      File whitelist_filter_output_allvariants_csv = file_prefix + ".all_variants.csv"
      File whitelist_filter_output_wl_csv = file_prefix + ".chip_wl_variants.csv"
      File whitelist_filter_output_manual_review_csv = file_prefix + ".chip_manual_review_variants.csv"
      File whitelist_filter_output_putative_wl_csv = file_prefix + ".chip_putative_wl_variants.csv"
      File whitelist_filter_output_putative_manual_review_csv = file_prefix + ".chip_putative_manual_review_variants.csv"
    }
}

# task Funcotate {
#      input {
#        File ref_fasta
#        File ref_fai
#        File ref_dict
#        File input_vcf
#        File input_vcf_idx
#        String reference_version
#        String output_file_base_name
#        String output_format
#        Boolean compress
#        Boolean use_gnomad
#        # This should be updated when a new version of the data sources is released
#        # TODO: Make this dynamically chosen in the command.
#        File? data_sources_tar_gz = "gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz"
#        String? control_id
#        String? case_id
#        String? sequencing_center
#        String? sequence_source
#        String? transcript_selection_mode
#        File? transcript_selection_list
#        Array[String]? annotation_defaults
#        Array[String]? annotation_overrides
#        Array[String]? funcotator_excluded_fields
#        Boolean? filter_funcotations
#        File? interval_list

#        String? extra_args

#        # ==============
#        Runtime runtime_params
#        Int? disk_space   #override to request more disk than default small task params

#        # You may have to change the following two parameter values depending on the task requirements
#        Int default_ram_mb = 3000
#        # WARNING: In the workflow, you should calculate the disk space as an input to this task (disk_space_gb).  Please see [TODO: Link from Jose] for examples.
#        Int default_disk_space_gb = 100
#      }

#      # ==============
#      # Process input args:
#      String output_maf = output_file_base_name + ".maf"
#      String output_maf_index = output_maf + ".idx"
#      String output_vcf = output_file_base_name + if compress then ".vcf.gz" else ".vcf"
#      String output_vcf_idx = output_vcf +  if compress then ".tbi" else ".idx"
#      String output_file = if output_format == "MAF" then output_maf else output_vcf
#      String output_file_index = if output_format == "MAF" then output_maf_index else output_vcf_idx
#      String transcript_selection_arg = if defined(transcript_selection_list) then " --transcript-list " else ""
#      String annotation_def_arg = if defined(annotation_defaults) then " --annotation-default " else ""
#      String annotation_over_arg = if defined(annotation_overrides) then " --annotation-override " else ""
#      String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then " --remove-filtered-variants " else ""
#      String excluded_fields_args = if defined(funcotator_excluded_fields) then " --exclude-field " else ""
#      String interval_list_arg = if defined(interval_list) then " -L " else ""
#      String extra_args_arg = select_first([extra_args, ""])

#      String dollar = "$"

#      parameter_meta{
#       ref_fasta: {localization_optional: true}
#       ref_fai: {localization_optional: true}
#       ref_dict: {localization_optional: true}
#       input_vcf: {localization_optional: true}
#       input_vcf_idx: {localization_optional: true}
#      }

#      command <<<
#          set -e
#          export GATK_LOCAL_JAR=~{default="/gatk/gatk.jar" runtime_params.gatk_override}

#          # Extract our data sources:
#          echo "Extracting data sources..."
#          mkdir datasources_dir
#          tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1
#          DATA_SOURCES_FOLDER="$PWD/datasources_dir"

#          # Handle gnomAD:
#          if ~{use_gnomad} ; then
#              echo "Enabling gnomAD..."
#              for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do
#                  if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then
#                      cd ~{dollar}{DATA_SOURCES_FOLDER}
#                      tar -zvxf ~{dollar}{potential_gnomad_gz}
#                      cd -
#                  else
#                      echo "ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}" 1>&2
#                      false
#                  fi
#              done
#          fi

#          # Run Funcotator:
#          gatk --java-options "-Xmx~{runtime_params.command_mem}m" Funcotator \
#              --data-sources-path $DATA_SOURCES_FOLDER \
#              --ref-version ~{reference_version} \
#              --output-file-format ~{output_format} \
#              -R ~{ref_fasta} \
#              -V ~{input_vcf} \
#              -O ~{output_file} \
#              ~{interval_list_arg} ~{default="" interval_list} \
#              --annotation-default normal_barcode:~{default="Unknown" control_id} \
#              --annotation-default tumor_barcode:~{default="Unknown" case_id} \
#              --annotation-default Center:~{default="Unknown" sequencing_center} \
#              --annotation-default source:~{default="Unknown" sequence_source} \
#              ~{"--transcript-selection-mode " + transcript_selection_mode} \
#              ~{transcript_selection_arg}~{default="" sep=" --transcript-list " transcript_selection_list} \
#              ~{annotation_def_arg}~{default="" sep=" --annotation-default " annotation_defaults} \
#              ~{annotation_over_arg}~{default="" sep=" --annotation-override " annotation_overrides} \
#              ~{excluded_fields_args}~{default="" sep=" --exclude-field " funcotator_excluded_fields} \
#              ~{filter_funcotations_args} \
#              ~{extra_args_arg}
#          # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:
#          if [[ "~{output_format}" == "MAF" ]] ; then
#             touch ~{output_maf_index}
#          fi
#      >>>

#     runtime {
#         docker: runtime_params.gatk_docker
#         bootDiskSizeGb: runtime_params.boot_disk_size
#         memory: runtime_params.machine_mem + " MB"
#         mem_mb: runtime_params.machine_mem
#         disks: "local-disk " + select_first([disk_space, runtime_params.disk]) + " HDD"
#         preemptible: runtime_params.preemptible
#         maxRetries: runtime_params.max_retries
#         cpu: runtime_params.cpu
#     }

#      output {
#          File funcotated_output_file = "~{output_file}"
#          File funcotated_output_file_index = "~{output_file_index}"
#      }
# }
