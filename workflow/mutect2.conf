include required(classpath("application"))

backend {
  default = SGE

  providers {
    SGE {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 100

        runtime-attributes = """
        Int cpu = 2
        Int mem_mb
        String? sge_queue
        String? sge_project
        String? docker
        String docker_shell = "/bin/bash"
        Int? bootDiskSizeGb
        String? disks
        Int? preemptible
        Int? maxRetries
        String bind_cmd = ""
        """

        submit = """
        qsub \
        -q short.q,long.q \
        -terse \
        -V \
        -b y \
        -N ${job_name} \
        -wd ${cwd} \
        -o ${out} \
        -e ${err} \
        -pe smp ${cpu} \
        ${"-l mem_free=" + mem_mb + "M" + " -l h_vmem=" + mem_mb + "M"} \
        ${"-q " + sge_queue} \
        ${"-P " + sge_project} \
        /usr/bin/env bash ${script}
        """

        submit-docker = """
        # Ensure SINGULARITY_CACHEDIR is set; else use a default based on user's home
        if [ -z $SINGULARITY_CACHEDIR ]
          then CACHE_DIR=$HOME/.singularity/cache
          else CACHE_DIR=$SINGULARITY_CACHEDIR
        fi
        # Ensure cache dir exists so lock file can be created
        mkdir -p $CACHE_DIR
        LOCK_FILE=$CACHE_DIR/singularity_pull_flock
        # Create an exclusive filelock
        flock --exclusive --timeout 900 $LOCK_FILE \
          singularity exec --containall docker://${docker} \
          echo "Successfully pulled ${docker}!"
        # Create wrapper script around docker command
        echo '#!/bin/bash' > ${cwd}/sge_docker_script.sh
        echo singularity exec --containall --bind ${cwd}:${docker_cwd} ${bind_cmd} ${'docker://' + docker} ${docker_shell} ${docker_script} >> ${cwd}/sge_docker_script.sh
        # Submit job
        qsub \
        -q short.q,long.q \
        -terse \
        -V \
        -b y \
        -N ${job_name} \
        -wd ${cwd} \
        -o ${out} \
        -e ${err} \
        -pe smp ${cpu} \
        ${"-l mem_free=" + mem_mb + "M" + " -l h_vmem=" + mem_mb + "M"} \
        ${"-q " + sge_queue} \
        ${"-P " + sge_project} \
        /usr/bin/env bash ${cwd}/sge_docker_script.sh
        """

        job-id-regex = "(\\d+)"

        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
      }
    }
    singularity {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        run-in-background = true
        runtime-attributes = """
        String? docker
        String docker_shell = "/bin/bash"
        String bind_cmd = ""
        """
        submit-docker = """
        singularity exec --containall --bind ${cwd}:${docker_cwd} ${bind_cmd} ${'docker://' + docker} ${docker_shell} ${docker_script}
        """
      }
    }
    PAPIv2 {
      actor-factory = "cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"
      config {
        # Google project
        project = "pb-dev-312200"

        # Base bucket for workflow executions
        root = "gs://somvar_tests/cromwell/executions"

        # Make the name of the backend used for call caching purposes insensitive to the
        # PAPI version.
        name-for-call-caching-purposes: PAPI

        # Emit a warning if jobs last longer than this amount of time. This might indicate that
        # something got stuck in PAPI.
        slow-job-warning-time: 24 hours

        # Number of workers to assign to PAPI requests
        request-workers = 3

        genomics {
          # A reference to an auth defined in the `google` stanza at the top. This auth is used
          # to create pipelines and manipulate auth JSONs.
          auth = "application_default"

          # Endpoint for APIs, no reason to change this unless directed by Google.
          endpoint-url = "https://lifesciences.googleapis.com/"

          # Currently Cloud Life Sciences API is available only in the US, Europe and Asia regions.
          # This might change in the future, the most up-to-date list is available here:
          # https://cloud.google.com/life-sciences/docs/concepts/locations
          # Note that this is only used to store metadata about the pipeline operation.
          # Worker VMs can be scheduled in any region (see default-zones).
          location = "us-central1"
        }

        filesystems {
          gcs {
            auth = "application_default"
            project = "pb-dev-312200"
          }
        }

        default-runtime-attributes {
          cpu: 1
          failOnStderr: false
          continueOnReturnCode: 0
          memory: "2048 MB"
          bootDiskSizeGb: 10
          # Allowed to be a String, or a list of Strings
          disks: "local-disk 10 SSD"
          noAddress: false
          preemptible: 1

          # The zones to schedule worker VMs in. These should be colocated with
          # the regions of your data buckets.
          # https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#zones
          zones = "australia-southeast1-a australia-southeast1-b australia-southeast1-c"
        }

        reference-disk-localization-manifest-files = ["gs://gcp-public-data--broad-references/refdisk_manifest.json"]
      }
    }
  }
}

google {
  application-name = "cromwell"
  auths = [
    {
      name = "application_default"
      scheme = "application_default"
    }
  ]
}

engine {
  filesystems {
    gcs {
      auth = "application_default"
      project = "pb-dev-312200"
    }
  }
}

# Enable call-caching
call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

# Configure web service
webservice {
  port = CROMWELL_PORT_TO_SED
  interface = localhost
}

# Use a MySQL database
database {
  profile = "slick.jdbc.MySQLProfile$"
  #driver = "slick.driver.MySQLDriver$"
  db {
    url = "jdbc:mysql://DBHOST_TO_SED:DBPORT_TO_SED/DBNAME_TO_SED?useSSL=false&rewriteBatchedStatements=true&useLegacyDatetimeCode=false&serverTimezone=Australia/Sydney"
    user = "cromwell_user"
    password = "12345678!1aA"
    driver = "com.mysql.cj.jdbc.Driver"
    # needed a bigger timeout for big jc jobs, with 10k tasks
    connectionTimeout = 5000
  }
}
